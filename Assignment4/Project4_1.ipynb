{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel:\n",
    "    def __init__(self, vocab_size, embedding_dim=256, num_blocks = 1, num_heads = 2, ff_dim=256, max_len=20, dropout_rate=0.1):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.max_len = max_len\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def TransformerBlock(self,inputs):\n",
    "\n",
    "        mha = tf.keras.layers.MultiHeadAttention(num_heads=self.num_heads,key_dim=self.embedding_dim, name=\"mha\")(inputs,inputs, use_causal_mask = True)\n",
    "        print(mha)\n",
    "        dropout1 = tf.keras.layers.Dropout(rate=self.dropout_rate, name='dropout1')(mha)\n",
    "        print(dropout1)\n",
    "        # add1 = tf.keras.layers.Add()([inputs, dropout1])\n",
    "\n",
    "        norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6,name = \"norm1\")(inputs + dropout1)\n",
    "        print(norm1)\n",
    "        fc1 = tf.keras.layers.Dense(self.ff_dim,\"relu\",name='fc1')(norm1)\n",
    "        print(fc1)\n",
    "        fc2 = tf.keras.layers.Dense(self.embedding_dim,\"relu\",name='fc2')(fc1)\n",
    "        print(fc2)\n",
    "        dropout2 = tf.keras.layers.Dropout(rate=self.dropout_rate,name='dropout2')(fc2)\n",
    "        print(dropout2)\n",
    "        # add2 = tf.keras.layers.Add()([norm1,dropout2])\n",
    "        norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6,name='norm2')(norm1+dropout2)\n",
    "        print(norm2)\n",
    "\n",
    "        return norm2\n",
    "    \n",
    "    def EmbeddingLayer(self, inputs):\n",
    "        word_embedding = tf.keras.layers.Embedding(self.vocab_size,self.embedding_dim,name=\"word_embedding\")(inputs)\n",
    "        pos_embedding = tf.keras.layers.Embedding(self.max_len,self.embedding_dim,name=\"pos_embedding\")(tf.range(self.max_len))\n",
    "\n",
    "        add1 = tf.keras.layers.Add(name=\"add1\")([word_embedding,pos_embedding])\n",
    "\n",
    "        return add1\n",
    "    \n",
    "    def create_model(self):\n",
    "        inputs = layers.Input(shape=(self.max_len,), name='inputs')\n",
    "        print(inputs)\n",
    "        embedding = self.EmbeddingLayer(inputs)\n",
    "        print(embedding)\n",
    "        tmp = embedding\n",
    "        for i in range(self.num_blocks):\n",
    "            tmp = self.TransformerBlock(tmp)\n",
    "\n",
    "        tmp = layers.Flatten()(tmp)\n",
    "\n",
    "        outputs = layers.Dense(self.vocab_size, activation='softmax', name='outputs')(tmp)\n",
    "        print(outputs)\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self, filename, sequence_length):\n",
    "        with open(filename, 'r') as file:\n",
    "            self.text = file.read()\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "    def prep_text(self):\n",
    "        self.text = re.sub(r'[^\\w\\s]', '', self.text)\n",
    "        # self.text = re.sub(r\"[^a-zA-Z0-9\\n]+\", ' ', self.text)\n",
    "        # self.text = re.sub(r\"(\\s+'\\s)|(\\s+'$)|(^'\\s)\", ' ', self.text)\n",
    "        # self.text = self.text.replace('\\r', '').replace('\\t', '    ').replace('\\f', '')\n",
    "    \n",
    "    def tokenize_text(self):\n",
    "        self.vocab = np.unique(list(self.text))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.char_to_idx = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n",
    "        self.text = np.array([self.char_to_idx[c] for c in self.text])\n",
    "        \n",
    "    def create_dataset(self):\n",
    "        #split the tokenized data into sequences of length seq_len, return the sequences and vocab\n",
    "        self.prep_text()\n",
    "        self.tokenize_text()\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(len(self.text) - self.sequence_length - 1):\n",
    "            x.append(self.text[i:i+self.sequence_length])\n",
    "            y.append(self.text[i+1:i+self.sequence_length+1])\n",
    "        return x, y, self.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateText():\n",
    "    def __init__(self, model, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.model = model\n",
    "\n",
    "    \n",
    "    def generate_text(self, start_string, num_generate=100):\n",
    "        #generate text using the model and vocab, start with the start_string and generate num_generate words\n",
    "        #use the model to predict the next word, then add it to the input and predict the next word, repeat until num_generate words have been generated\n",
    "\n",
    "        #convert the start_string to a list of numbers using the vocab\n",
    "        start_tokens = [np.where(self.vocab == word)[0][0] for word in start_string.split(' ')]\n",
    "        \n",
    "        for i in range(num_generate):\n",
    "            #use the model to predict the next word\n",
    "            prediction = self.model.predict(start_tokens)\n",
    "            #add the predicted word to the input\n",
    "            next_token = np.argmax(prediction)\n",
    "            start_tokens.append(next_token)\n",
    "        #convert the list of numbers back to a string using the vocab\n",
    "        return ' '.join([self.vocab[i] for i in start_tokens])\n",
    "    \n",
    "    def generate_random_text(self, num_generate=100):\n",
    "        #generate text using the model and vocab, start with a random word and generate num_generate words\n",
    "\n",
    "        #choose a random word from the vocab as the start_string\n",
    "        start_string = np.random.choice(self.vocab)\n",
    "        return self.generate_text(start_string, num_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model while periodically generating text to show progress\n",
    "def train_model(model, vocab, x, y, epochs=50, verbose=1):\n",
    "\n",
    "    # gen_text = GenerateText(model, vocab)\n",
    " \n",
    "    print(\"BEFORE FIT\")\n",
    "    model = model.fit(x, y,verbose=2,epochs=epochs,use_multiprocessing=True)\n",
    "    print(\"PASSED FIT\")\n",
    "        # if verbose == 1:\n",
    "        #     #generate text using the model\n",
    "        #     print(f'Epoch {i}')\n",
    "        #     print(gen_text.generate_random_text())\n",
    "        #     print('\\n\\n')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataSet('beatles.txt', 20)\n",
    "x, y, vocab = dataset.create_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(len(vocab))\n",
    "model = model.create_model()\n",
    "\n",
    "# Train the model\n",
    "model = train_model(model=model, vocab=vocab, x=x, y=y, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
