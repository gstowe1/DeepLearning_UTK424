{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2adb05c1",
   "metadata": {},
   "source": [
    "# Project 4\n",
    "## Students:\n",
    " > [Eli Carter]\n",
    " > [Gabriel Stowe]\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563a5a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-28 17:10:33.434389: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ebf08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)# you may want to upgrade to 2.10.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a95a42",
   "metadata": {},
   "source": [
    "### Please Use Markdown\n",
    "> for markdown, see here: https://www.ibm.com/docs/en/watson-studio-local/1.2.3?topic=notebooks-markdown-jupyter-cheatsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae40d9",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2493f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel():\n",
    "    def __init__(self, vocab_size, embed_dim=256, num_heads=2, num_blocks=1, ff_dim=256, maxlen=64, rate=0.1):\n",
    "        #initailize variables\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "        self.ff_dim = ff_dim\n",
    "        self.maxlen = maxlen\n",
    "        self.rate = rate\n",
    "\n",
    "    def TransformerBlock(self, inputs):\n",
    "        #create the transformer block as discribed in the writeup, use the Keras functional API (https://keras.io/guides/functional_api/)\n",
    "        #MultiHeadAttention layer, specifiy 'use_causal_mask=True' (https://keras.io/api/layers/attention_layers/multi_head_attention/)\n",
    "        #LayerNormalization layer, specifiy 'epsilon=1e-6' (https://keras.io/api/layers/normalization_layers/layer_normalization/)\n",
    "        #Use the rate variable for the dropout layers\n",
    "        mha = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embed_dim/self.num_heads, use_causal_mask=True)(inputs)\n",
    "        d1 = layers.Dropout(self.rate)(mha)\n",
    "        n1 = layers.LayerNormalization(epsilon=1e-6)(d1 + input)\n",
    "        fc1 = layers.Dense(self.ff_dim, activation='relu')(n1)\n",
    "        fc2 = layers.Dense(self.ff_dim, activation='relu')(fc1)\n",
    "        d2 = layers.Dropout(self.rate)(fc2)\n",
    "        n2 = layers.LayerNormalization(epsilon=1e-6)(d2 + n1)\n",
    "        return n2\n",
    "\n",
    "\n",
    "    \n",
    "    def EmbeddingLayer(self, inputs):\n",
    "        #create the embedding layer\n",
    "        #create (1) an embedding for the tokens and (2) an embedding for the positions\n",
    "        #you can use https://keras.io/api/layers/core_layers/embedding/ Embedding class\n",
    "        #you can use tf.range to enocde positions\n",
    "        #add (1) and (2) and return the layer\n",
    "        toke_embedding = layers.Embedding(input_dim=self.vocab_size, output_dim=self.embed_dim)(inputs)\n",
    "        pos_embedding = layers.Embedding(input_dim=self.maxlen, output_dim=self.embed_dim)(tf.range(self.maxlen))\n",
    "        return toke_embedding + pos_embedding\n",
    "        #return layers.TokenAndPositionEmbedding(self.maxlen, self.vocab_size, self.embed_dim)(inputs)\n",
    "    \n",
    "    def create_model(self):\n",
    "        #combine the EmbeddingLayer and num_blocks TransformerBlocks to create the model, use the Keras functional API (https://keras.io/guides/functional_api/)\n",
    "        #use the SparseCategoricalCrossentropy loss function (https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class)\n",
    "\n",
    "        inputs = layers.Input(shape=(self.maxlen,))\n",
    "        embedding = self.EmbeddingLayer(inputs)\n",
    "        tmp = embedding\n",
    "        for i in range(self.num_blocks):\n",
    "            tmp = self.TransformerBlock(tmp)\n",
    "\n",
    "        outputs = layers.Dense(self.vocab_size, activation='softmax')(tmp)\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        #print(model.summary())\n",
    "        #keras.utils.plot_model(model, \"model.png\", show_shapes=True)\n",
    "        #model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad747b",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "227111a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet():\n",
    "    def __init__(self, filename, len):\n",
    "        #load the text from the file\n",
    "        self.text = open(filename, 'r').read()\n",
    "        self.len = len\n",
    "        \n",
    "\n",
    "    def prep_text(self):\n",
    "        #remove all punctuation\n",
    "        self.text = re.sub(r'[^\\w\\s]', '', self.text)\n",
    "        #remove all special characters\n",
    "        self.text = re.sub(r'[^a-zA-Z0-9\\s]', '', self.text)\n",
    "        #replace all whitespaces except for \\n with a space\n",
    "        self.text = re.sub(r'[^\\S\\n]+', ' ', self.text)\n",
    "        #replace all \\n with a space, newline, then space\n",
    "        self.text = re.sub(r'\\n', ' \\n ', self.text)\n",
    "        \n",
    "    def tokenize_text(self):\n",
    "        #seperate into words, create a vocab and convert the text to a list of numbers using the vocab such that each unique word is represented by its own number\n",
    "        self.text = self.text.split(' ')\n",
    "        self.vocab = np.unique(self.text)\n",
    "        self.text = [np.where(self.vocab == word)[0][0] for word in self.text]\n",
    "\n",
    "    def create_dataset(self):\n",
    "        #split the tokenized data into sequences of length len, return the sequences and vocab\n",
    "        self.prep_text()\n",
    "        self.tokenize_text()\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(len(self.text) - self.len - 1):\n",
    "            x.append(self.text[i:i+self.len])\n",
    "            y.append(self.text[i+1:i+self.len+1])\n",
    "        return x, y, self.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3a399",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ffe1274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateText():\n",
    "    def __init__(self, model, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.model = model\n",
    "\n",
    "    \n",
    "    def generate_text(self, start_string, num_generate=100):\n",
    "        #generate text using the model and vocab, start with the start_string and generate num_generate words\n",
    "        #use the model to predict the next word, then add it to the input and predict the next word, repeat until num_generate words have been generated\n",
    "\n",
    "        #convert the start_string to a list of numbers using the vocab\n",
    "        start_tokens = [np.where(self.vocab == word)[0][0] for word in start_string.split(' ')]\n",
    "        \n",
    "        for i in range(num_generate):\n",
    "            #use the model to predict the next word\n",
    "            prediction = self.model.predict(start_tokens)\n",
    "            #add the predicted word to the input\n",
    "            start_tokens.append(np.argmax(prediction))\n",
    "        #convert the list of numbers back to a string using the vocab\n",
    "        return ' '.join([self.vocab[i] for i in start_tokens])\n",
    "    \n",
    "    def generate_random_text(self, num_generate=100):\n",
    "        #generate text using the model and vocab, start with a random word and generate num_generate words\n",
    "\n",
    "        #choose a random word from the vocab as the start_string\n",
    "        start_string = np.random.choice(self.vocab)\n",
    "        return self.generate_text(start_string, num_generate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0bd9d",
   "metadata": {},
   "source": [
    "## Task 4: Model Traning and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b59dd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model while periodically generating text to show progress\n",
    "def train_model(model, vocab, x, y, epochs=50, verbose=1):\n",
    "    gen_text = GenerateText(model, vocab)\n",
    "    for i in range(epochs):\n",
    "        #train the model\n",
    "        model.fit(x, y)\n",
    "        \n",
    "        if i % 10 == 0 and verbose == 1:\n",
    "            #generate text using the model\n",
    "            print(f'Epoch {i}')\n",
    "            print(gen_text.generate_random_text())\n",
    "            print('\\n\\n')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f390b808",
   "metadata": {},
   "source": [
    "## Running the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17d89c2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "('Keyword argument not understood:', 'use_causal_mask')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Create the model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[39m=\u001b[39m TransformerModel(\u001b[39mlen\u001b[39m(vocab))\n\u001b[0;32m----> 6\u001b[0m model\u001b[39m.\u001b[39;49mcreate_model()\n",
      "Cell \u001b[0;32mIn[8], line 47\u001b[0m, in \u001b[0;36mTransformerModel.create_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m tmp \u001b[39m=\u001b[39m embedding\n\u001b[1;32m     46\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_blocks):\n\u001b[0;32m---> 47\u001b[0m     tmp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mTransformerBlock(tmp)\n\u001b[1;32m     49\u001b[0m outputs \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mDense(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m)(tmp)\n\u001b[1;32m     50\u001b[0m model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mModel(inputs\u001b[39m=\u001b[39minputs, outputs\u001b[39m=\u001b[39moutputs)\n",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m, in \u001b[0;36mTransformerModel.TransformerBlock\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mTransformerBlock\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[1;32m     13\u001b[0m     \u001b[39m#create the transformer block as discribed in the writeup, use the Keras functional API (https://keras.io/guides/functional_api/)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[39m#MultiHeadAttention layer, specifiy 'use_causal_mask=True' (https://keras.io/api/layers/attention_layers/multi_head_attention/)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[39m#LayerNormalization layer, specifiy 'epsilon=1e-6' (https://keras.io/api/layers/normalization_layers/layer_normalization/)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[39m#Use the rate variable for the dropout layers\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     mha \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39;49mMultiHeadAttention(num_heads\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads, key_dim\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim\u001b[39m/\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads, use_causal_mask\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)(inputs)\n\u001b[1;32m     18\u001b[0m     d1 \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mDropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrate)(mha)\n\u001b[1;32m     19\u001b[0m     n1 \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mLayerNormalization(epsilon\u001b[39m=\u001b[39m\u001b[39m1e-6\u001b[39m)(d1 \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py38/lib/python3.8/site-packages/keras/layers/attention/multi_head_attention.py:252\u001b[0m, in \u001b[0;36mMultiHeadAttention.__init__\u001b[0;34m(self, num_heads, key_dim, value_dim, dropout, use_bias, output_shape, attention_axes, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    235\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    236\u001b[0m     num_heads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    251\u001b[0m ):\n\u001b[0;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    253\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_masking \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_heads \u001b[39m=\u001b[39m num_heads\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/trackable/base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py38/lib/python3.8/site-packages/keras/engine/base_layer.py:341\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m allowed_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    331\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minput_dim\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    332\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimplementation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    339\u001b[0m }\n\u001b[1;32m    340\u001b[0m \u001b[39m# Validate optional keyword arguments.\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m generic_utils\u001b[39m.\u001b[39;49mvalidate_kwargs(kwargs, allowed_kwargs)\n\u001b[1;32m    343\u001b[0m \u001b[39m# Mutable properties\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[39m# Indicates whether the layer's weights are updated during training\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39m# and whether the layer's updates are run during training.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\n\u001b[1;32m    347\u001b[0m     \u001b[39misinstance\u001b[39m(trainable, \u001b[39mbool\u001b[39m)\n\u001b[1;32m    348\u001b[0m     \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    351\u001b[0m     )\n\u001b[1;32m    352\u001b[0m ):\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py38/lib/python3.8/site-packages/keras/utils/generic_utils.py:514\u001b[0m, in \u001b[0;36mvalidate_kwargs\u001b[0;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mfor\u001b[39;00m kwarg \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    513\u001b[0m     \u001b[39mif\u001b[39;00m kwarg \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allowed_kwargs:\n\u001b[0;32m--> 514\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(error_message, kwarg)\n",
      "\u001b[0;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'use_causal_mask')"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "dataset = DataSet('beatles.txt', 20)\n",
    "x, y, vocab = dataset.create_dataset()\n",
    "# Create the model\n",
    "model = TransformerModel(len(vocab))\n",
    "model.create_model()\n",
    "# Train the model\n",
    "#model = train_model(model, vocab, x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658fa81b",
   "metadata": {},
   "source": [
    "\n",
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b723a2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855b442",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c41dc86",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812e555",
   "metadata": {},
   "source": [
    "## How to Run Code\n",
    "\n",
    "Please include any special libraries and list your tf version here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
