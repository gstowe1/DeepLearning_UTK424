{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2adb05c1",
   "metadata": {},
   "source": [
    "# Project 4\n",
    "## Students:\n",
    " > [Eli Carter]\n",
    " > [Gabriel Stowe]\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "563a5a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2ebf08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)# you may want to upgrade to 2.10.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a95a42",
   "metadata": {},
   "source": [
    "### Please Use Markdown\n",
    "> for markdown, see here: https://www.ibm.com/docs/en/watson-studio-local/1.2.3?topic=notebooks-markdown-jupyter-cheatsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae40d9",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2493f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel():\n",
    "    def __init__(self, vocab_size, embed_dim=256, num_heads=2, num_blocks=1, ff_dim=256, maxlen=100, rate=0.1):\n",
    "        #initailize variables\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "        self.ff_dim = ff_dim\n",
    "        self.maxlen = maxlen\n",
    "        self.rate = rate\n",
    "\n",
    "    def TransformerBlock(self, inputs):\n",
    "        #create the transformer block as discribed in the writeup, use the Keras functional API (https://keras.io/guides/functional_api/)\n",
    "        #MultiHeadAttention layer, specifiy 'use_causal_mask=True' (https://keras.io/api/layers/attention_layers/multi_head_attention/)\n",
    "        #LayerNormalization layer, specifiy 'epsilon=1e-6' (https://keras.io/api/layers/normalization_layers/layer_normalization/)\n",
    "        #Use the rate variable for the dropout layers\n",
    "        mha = tf.keras.layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embed_dim)(inputs, inputs, use_causal_mask=True)\n",
    "        d1 = layers.Dropout(self.rate)(mha)\n",
    "        n1 = layers.LayerNormalization(epsilon=1e-6)(d1 + inputs)\n",
    "        fc1 = layers.Dense(self.ff_dim, activation='relu')(n1)\n",
    "        fc2 = layers.Dense(self.ff_dim, activation='relu')(fc1)\n",
    "        d2 = layers.Dropout(self.rate)(fc2)\n",
    "        n2 = layers.LayerNormalization(epsilon=1e-6)(d2 + n1)\n",
    "        return n2\n",
    "\n",
    "\n",
    "    \n",
    "    def EmbeddingLayer(self, inputs):\n",
    "        #create the embedding layer\n",
    "        #create (1) an embedding for the tokens and (2) an embedding for the positions\n",
    "        #you can use https://keras.io/api/layers/core_layers/embedding/ Embedding class\n",
    "        #you can use tf.range to enocde positions\n",
    "        #add (1) and (2) and return the layer\n",
    "        toke_embedding = layers.Embedding(input_dim=self.vocab_size, output_dim=self.embed_dim)(inputs)\n",
    "        pos_embedding = layers.Embedding(input_dim=self.maxlen, output_dim=self.embed_dim)(tf.range(self.maxlen))\n",
    "        return toke_embedding + pos_embedding\n",
    "        #return layers.TokenAndPositionEmbedding(self.maxlen, self.vocab_size, self.embed_dim)(inputs)\n",
    "    \n",
    "    def create_model(self):\n",
    "        #combine the EmbeddingLayer and num_blocks TransformerBlocks to create the model, use the Keras functional API (https://keras.io/guides/functional_api/)\n",
    "        #use the SparseCategoricalCrossentropy loss function (https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class)\n",
    "\n",
    "        inputs = layers.Input(shape=(self.maxlen,))\n",
    "        embedding = self.EmbeddingLayer(inputs)\n",
    "        tmp = embedding\n",
    "        for i in range(self.num_blocks):\n",
    "            tmp = self.TransformerBlock(tmp)\n",
    "\n",
    "        outputs = layers.Dense(self.vocab_size, activation='softmax')(tmp)\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad747b",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "227111a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet():\n",
    "    def __init__(self, filename, len):\n",
    "        #load the text from the file\n",
    "        self.text = open(filename, 'r').read()\n",
    "        self.len = len\n",
    "        \n",
    "\n",
    "    def prep_text(self):\n",
    "        #remove all punctuation\n",
    "        self.text = re.sub(r'[^\\w\\s]', '', self.text)\n",
    "        #remove all special characters\n",
    "        self.text = re.sub(r'[^a-zA-Z0-9\\s]', '', self.text)\n",
    "        #replace all whitespaces except for \\n with a space\n",
    "        self.text = re.sub(r'[^\\S\\n]+', ' ', self.text)\n",
    "        #replace all \\n with a space, newline, then space\n",
    "        self.text = re.sub(r'\\n', ' \\n ', self.text)\n",
    "        \n",
    "    def tokenize_text(self):\n",
    "        #seperate into words, create a vocab and convert the text to a list of numbers using the vocab such that each unique word is represented by its own number\n",
    "        self.text = self.text.split(' ')\n",
    "        #remove all the empty strings ??????\n",
    "        c = self.text.count('')\n",
    "        for i in range(c):\n",
    "            self.text.remove('')\n",
    "        self.vocab = np.unique(self.text)\n",
    "        self.text = np.array([np.where(self.vocab == word)[0][0] for word in self.text])\n",
    "\n",
    "    def create_dataset(self):\n",
    "        #split the tokenized data into sequences of length len, return the sequences and vocab\n",
    "        self.prep_text()\n",
    "        self.tokenize_text()\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(len(self.text) - self.len - 1):\n",
    "            x.append(self.text[i:i+self.len])\n",
    "            y.append(self.text[i+1:i+self.len+1])\n",
    "        return np.array(x), np.array(y), self.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b3cce1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2585 1444 1957 1173   15    0 1986 1095   34    0  481    0 2252 1932\n",
      " 2435  251   13 1271 1271 2276   37    0 2585 1444 1957 1173 2585 1444\n",
      " 1957    0 2585 1444 1957 1173 2565    0 2585 1444 1957 1173 2585 1444\n",
      " 1957    0 2585 1444 1957 1173 2565    0 2585 1444 1957 1173 2585 1444\n",
      " 1957    0 2585 1444 1957 1173 2565    0]\n",
      "[1444 1957 1173   15    0 1986 1095   34    0  481    0 2252 1932 2435\n",
      "  251   13 1271 1271 2276   37    0 2585 1444 1957 1173 2585 1444 1957\n",
      "    0 2585 1444 1957 1173 2565    0 2585 1444 1957 1173 2585 1444 1957\n",
      "    0 2585 1444 1957 1173 2565    0 2585 1444 1957 1173 2585 1444 1957\n",
      "    0 2585 1444 1957 1173 2565    0  133]\n",
      "['\\n' '0' '1' ... 'zapped' 'zoo' 'zu']\n",
      "<class 'numpy.ndarray'>\n",
      "2595\n"
     ]
    }
   ],
   "source": [
    "test = DataSet('beatles.txt', 64)\n",
    "x, y, vocab = test.create_dataset()\n",
    "print(x[-1])\n",
    "print(y[-1])\n",
    "print(vocab)\n",
    "print(type(vocab))\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3a399",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6ffe1274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateText():\n",
    "    def __init__(self, model, vocab, maxlen):\n",
    "        self.vocab = vocab\n",
    "        self.model = model\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    \n",
    "    def generate_text(self, start_string, num_generate=150):\n",
    "        #generate text using the model and vocab, start with the start_string and generate num_generate words\n",
    "        #use the model to predict the next word, then add it to the input and predict the next word, repeat until num_generate words have been generated\n",
    "\n",
    "        #convert the start_string to a numpy list of numbers using the vocab \n",
    "        start_tokens = [np.where(self.vocab == word)[0][0] for word in start_string.split(' ')]\n",
    "        \n",
    "        for i in range(num_generate):\n",
    "            pad_len = self.maxlen - len(start_tokens)\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[-self.maxlen:]\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "            x = np.array([x])\n",
    "\n",
    "            #use the model to predict the next word\n",
    "            prediction = self.model.predict(x,verbose=0)[0][min([i,self.maxlen-1])]\n",
    "            #add the predicted word to the input\n",
    "            start_tokens.append(np.argmax(prediction))\n",
    "\n",
    "        #convert the list of numbers back to a string using the vocab\n",
    "        return ' '.join([self.vocab[i] for i in start_tokens])\n",
    "    \n",
    "    def generate_random_text(self, num_generate=150):\n",
    "        #generate text using the model and vocab, start with a random word and generate num_generate words\n",
    "\n",
    "        #choose a random word from the vocab as the start_string\n",
    "        start_string = np.random.choice(self.vocab)\n",
    "        return self.generate_text(start_string, num_generate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b8b09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edd0bd9d",
   "metadata": {},
   "source": [
    "## Task 4: Model Traning and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1b59dd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#Do not use fit verbose. This will tell what Epoch currently on. \n",
    "def train_model(model, vocab, x, y, epochs=50, verbose=0, maxlen=50, heads=0):\n",
    "    start_time = time.time()\n",
    "    for i in range(epochs):\n",
    "        #train the model\n",
    "        print(\"Epoch:\",i+1)\n",
    "        history = model.fit(x, y,verbose=verbose ,use_multiprocessing=True)\n",
    "        \n",
    "        if epochs == 1 or ((i+1) % 10 == 0):\n",
    "            with open(f'{heads}_{epochs}.txt', 'a') as file:\n",
    "                #generate text using the model\n",
    "                gen_text = GenerateText(model, vocab, maxlen)\n",
    "                file.write(f'Epoch {i+1}\\n')\n",
    "                file.write(\"Generated Text:\\n\")\n",
    "                file.write(gen_text.generate_random_text())\n",
    "                file.write(\"\\n\")\n",
    "                file.write(f\"Accuracy: {history.history['accuracy'][-1]} Loss: {history.history['loss'][-1]} Total Time: {time.time() - start_time} seconds\\n\")\n",
    "                \n",
    "                print(f\"Accuracy: {history.history['accuracy'][-1]} Loss: {history.history['loss'][-1]} Total Time: {time.time() - start_time} seconds\\n\")\n",
    "                \n",
    "                file.write('\\n\\n')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f390b808",
   "metadata": {},
   "source": [
    "## Running the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "17d89c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working: 1 epochs 2 heads\n",
      "Epoch: 1\n",
      "Accuracy: 0.5701628923416138 Loss: 1.9290947914123535 Total Time: 126.69433236122131 seconds\n",
      "\n",
      "Working: 1 epochs 4 heads\n",
      "Epoch: 1\n",
      "Accuracy: 0.5826221704483032 Loss: 1.8848795890808105 Total Time: 176.5903513431549 seconds\n",
      "\n",
      "Working: 50 epochs 2 heads\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Accuracy: 0.9351632595062256 Loss: 0.23857295513153076 Total Time: 1236.5649600028992 seconds\n",
      "\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Accuracy: 0.9435104727745056 Loss: 0.2053060084581375 Total Time: 2454.8429794311523 seconds\n",
      "\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Accuracy: 0.9469761252403259 Loss: 0.1913803666830063 Total Time: 3666.451151371002 seconds\n",
      "\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Accuracy: 0.9492838382720947 Loss: 0.1819354146718979 Total Time: 4907.882239341736 seconds\n",
      "\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Accuracy: 0.9507583379745483 Loss: 0.17581702768802643 Total Time: 6182.828451633453 seconds\n",
      "\n",
      "Working: 50 epochs 4 heads\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Accuracy: 0.9346465468406677 Loss: 0.23944103717803955 Total Time: 1743.1399652957916 seconds\n",
      "\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Accuracy: 0.9442896246910095 Loss: 0.20044802129268646 Total Time: 3471.9283435344696 seconds\n",
      "\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Accuracy: 0.9485242366790771 Loss: 0.1836659163236618 Total Time: 5206.406163215637 seconds\n",
      "\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Accuracy: 0.951178789138794 Loss: 0.17263750731945038 Total Time: 6951.060561656952 seconds\n",
      "\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Accuracy: 0.9526805877685547 Loss: 0.1659347265958786 Total Time: 8680.100900173187 seconds\n",
      "\n",
      "Working: 100 epochs 2 heads\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Accuracy: 0.9345349073410034 Loss: 0.24050100147724152 Total Time: 1263.5590879917145 seconds\n",
      "\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Accuracy: 0.9434142112731934 Loss: 0.2059769183397293 Total Time: 2527.231454849243 seconds\n",
      "\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Accuracy: 0.9468984603881836 Loss: 0.19161036610603333 Total Time: 3823.4732151031494 seconds\n",
      "\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Accuracy: 0.948948323726654 Loss: 0.1818699985742569 Total Time: 5113.965126037598 seconds\n",
      "\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Accuracy: 0.9502822756767273 Loss: 0.17662093043327332 Total Time: 6444.13707280159 seconds\n",
      "\n",
      "Epoch: 51\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Accuracy: 0.9512209892272949 Loss: 0.1727851778268814 Total Time: 7794.209453821182 seconds\n",
      "\n",
      "Epoch: 61\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Accuracy: 0.952190101146698 Loss: 0.16844986379146576 Total Time: 9152.326124668121 seconds\n",
      "\n",
      "Epoch: 71\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Accuracy: 0.9530696868896484 Loss: 0.16522186994552612 Total Time: 10516.466124773026 seconds\n",
      "\n",
      "Epoch: 81\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n",
      "Accuracy: 0.953210175037384 Loss: 0.16383114457130432 Total Time: 11906.416963100433 seconds\n",
      "\n",
      "Epoch: 91\n",
      "Epoch: 92\n",
      "Epoch: 93\n",
      "Epoch: 94\n",
      "Epoch: 95\n",
      "Epoch: 96\n",
      "Epoch: 97\n",
      "Epoch: 98\n",
      "Epoch: 99\n",
      "Epoch: 100\n",
      "Accuracy: 0.9538812637329102 Loss: 0.16187456250190735 Total Time: 13323.360947847366 seconds\n",
      "\n",
      "Working: 100 epochs 4 heads\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Accuracy: 0.9337494969367981 Loss: 0.24203984439373016 Total Time: 2003.378966331482 seconds\n",
      "\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Accuracy: 0.9444728493690491 Loss: 0.20047517120838165 Total Time: 4011.8648521900177 seconds\n",
      "\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Accuracy: 0.9484562873840332 Loss: 0.18368402123451233 Total Time: 6022.569329500198 seconds\n",
      "\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Accuracy: 0.9509081244468689 Loss: 0.1735197752714157 Total Time: 8073.466633081436 seconds\n",
      "\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Accuracy: 0.9524628520011902 Loss: 0.16688460111618042 Total Time: 10083.987149953842 seconds\n",
      "\n",
      "Epoch: 51\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Accuracy: 0.9538807272911072 Loss: 0.16047470271587372 Total Time: 12224.03370809555 seconds\n",
      "\n",
      "Epoch: 61\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Accuracy: 0.9546501636505127 Loss: 0.1578049212694168 Total Time: 14423.80218219757 seconds\n",
      "\n",
      "Epoch: 71\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Accuracy: 0.9553387761116028 Loss: 0.15403766930103302 Total Time: 16626.122886657715 seconds\n",
      "\n",
      "Epoch: 81\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n",
      "Accuracy: 0.9558858275413513 Loss: 0.15207451581954956 Total Time: 18803.7050178051 seconds\n",
      "\n",
      "Epoch: 91\n",
      "Epoch: 92\n",
      "Epoch: 93\n",
      "Epoch: 94\n",
      "Epoch: 95\n",
      "Epoch: 96\n",
      "Epoch: 97\n",
      "Epoch: 98\n",
      "Epoch: 99\n",
      "Epoch: 100\n",
      "Accuracy: 0.9564926028251648 Loss: 0.14970870316028595 Total Time: 21028.842777252197 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "dataset = DataSet('beatles.txt', 50)\n",
    "x, y, vocab = dataset.create_dataset()\n",
    "# Create the model\n",
    "# Train the model\n",
    "for epoch in [1,50,100]:\n",
    "    for head in [2,4]:\n",
    "        print(f\"Working: {epoch} epochs {head} heads\")\n",
    "        my_model = TransformerModel(len(vocab), num_heads=head, maxlen=50)\n",
    "        compiled_model = my_model.create_model()\n",
    "\n",
    "        trained_model = train_model(compiled_model, vocab, x, y, epoch, heads=head)\n",
    "        # trained_model.save('saved_model/64_50_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658fa81b",
   "metadata": {},
   "source": [
    "\n",
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b723a2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75725ea9",
   "metadata": {},
   "source": [
    "In this project, we will be building a Transformer-based neural network to write Beatles songs. The goal is to frame the problem as a many-to-many task, where we are trying to predict a series of words. We will be using a text file that includes lyrics from 246 Beatles songs, which are concatenated with each other and treated as one long sequence. The dataset is taken from the following website: http://beatlesnumber9.com/lyrics.html.\n",
    "\n",
    "Our network is built on a Transformer architecture that was laid out in the project write up. This Transformer architecture consisits of a TransformerModel class, which will include an init method, a TransformerBlock method, an EmbeddingLayer method, and a create_model method. In addition to, we created a DataSet class, which will be responsible for loading the text and generating sequences for training. Finally, we created a GenerateText class, which will be responsible for using the model to generate text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855b442",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbfaca34",
   "metadata": {},
   "source": [
    "For this particular project, the training data set is considered pretty small. While the the model is able to predict and form \"beatle songs\", the model is far from pefect. Down below is the result from running the model with epochs 1, 50 and 100 with attention heads of 2 and 4 for each. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731ca6ef",
   "metadata": {},
   "source": [
    "### 1 Epcoh With 2 Attention Heads:\n",
    "```\n",
    "Accuracy: 0.57\n",
    "Loss: 1.92\n",
    "Total Time To Complete: 126.69 Seconds\n",
    "\n",
    "Generated Text:\n",
    "lagoon \n",
    " didnt anybody tell her \n",
    " sundays on the phone to monday \n",
    " tuesdays on the phone to me \n",
    " she said shed always been a dancer \n",
    " she worked at 15 clubs a day \n",
    " and though she thought i knew the answer \n",
    " and though she knew the answer \n",
    " well i could not have a day \n",
    " well i knew the answer \n",
    " well i could not have a fool on the hill \n",
    " well i never seems to the hill \n",
    " man i tell you man with the foolish grin is keeping perfectly still \n",
    " and never know him \n",
    " and they can see \n",
    " they can see \n",
    " they can see \n",
    " that hes just a fool that hes just a fool \n",
    " and he never gives an answer \n",
    " and the fool on the hill \n",
    " sees the sun going down \n",
    " and the\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd1ea9a4",
   "metadata": {},
   "source": [
    "### 50 Epcohs With 2 Attention Heads:\n",
    "```\n",
    "Accuracy: 0.95\n",
    "Loss: 0.18\n",
    "Total Time To Complete: 6182.82 Seconds\n",
    "\n",
    "Generated Text:\n",
    "pornographic priestess \n",
    " boy you been a naughty girl you let your knickers down \n",
    " i am the eggman they are the eggmen \n",
    " i am the walrus goo goo gjoob \n",
    " sitting in an english garden waiting for the sun \n",
    " if the sun dont come you get a tan \n",
    " from standing in the english rain \n",
    " i am the eggman they are the eggmen \n",
    " i am the walrus goo goo gjoob ggoo goo gjoob \n",
    " expert textpert choking smokers \n",
    " dont you think the joker laughs at you \n",
    " see how they smile like pigs in a sty \n",
    " see how they snide \n",
    " im crying \n",
    " semolina pilchard climbing up the eiffel tower \n",
    " elementary penguin singing hari krishna \n",
    " man you should have seen them kicking edgar allan poe \n",
    " i am the eggman they are the eggmen \n",
    " i am the walrus goo goo\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79b2beb1",
   "metadata": {},
   "source": [
    "### 100 Epcohs With 2 Attention Heads:\n",
    "```\n",
    "Accuracy: 0.95\n",
    "Loss: 0.16\n",
    "Total Time To Complete: 13323.36 Seconds\n",
    "\n",
    "Generated Text:\n",
    "ooh \n",
    " i dug a pony \n",
    " well you can syndicate any boat you row \n",
    " yes you can syndicate any boat you row \n",
    " i told you all i want is you \n",
    " evrything has got to bejust like you want it to \n",
    " because \n",
    " dig it \n",
    " like a rolling stone \n",
    " a like a rolling stone \n",
    " like the fbi and the cia \n",
    " and the bbcbb king \n",
    " and doris day \n",
    " matt busby \n",
    " dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig it dig\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65df3200",
   "metadata": {},
   "source": [
    "### 1 Epcoh With 4 Attention Heads:\n",
    "```\n",
    "Accuracy: 0.58\n",
    "Loss: 1.88\n",
    "Total Time To Complete: 176.59 Seconds\n",
    "\n",
    "Generated Text:\n",
    "hates to see me say \n",
    " i see me cry \n",
    " i see you say \n",
    " i see you say \n",
    " i will hear you say that i will hear you say \n",
    " youll be happy cause you say \n",
    " youll be happy \n",
    " i will never leave me say \n",
    " if you and if you know that i will love me \n",
    " if you dont want to last \n",
    " i will remember you will love you by \n",
    " i will love her \n",
    " i think of things she said \n",
    " when i get her \n",
    " for her \n",
    " shes mine \n",
    " and i think of things she said her \n",
    " she said her things she said her \n",
    " she said her \n",
    " only to be dead \n",
    " and i know that living with me is to be free \n",
    " is out tonight \n",
    " shes thinking of her \n",
    " shes got\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e30074b",
   "metadata": {},
   "source": [
    "### 50 Epcohs With 4 Attention Heads:\n",
    "```\n",
    "Accuracy: 0.95\n",
    "Loss: 0.16\n",
    "Total Time To Complete: 8680.10 Seconds\n",
    "\n",
    "Generated Text:\n",
    "institution \n",
    " well you know \n",
    " you better free your mind instead \n",
    " but if you go carrying pictures of chairman mao \n",
    " you aint going to make it with anyone anyhow \n",
    " dont you know know its gonna be alright \n",
    " alright \n",
    " alright \n",
    " alright \n",
    " repeat till fade \n",
    " rock and roll music \n",
    " just let me hear some of that rock and roll music \n",
    " any old way you choose it \n",
    " its got a back beat you cant lose it \n",
    " any old time you use it \n",
    " its gotta be rock roll music \n",
    " if you wanna dance with me \n",
    " if you wanna dance with me \n",
    " way down south they gave a jubilee \n",
    " the jokey folks they had a jamboree \n",
    " theyre drinkin home brew from a water cup \n",
    " the folks dancin got all shook up \n",
    " and started playin that\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ace1e40",
   "metadata": {},
   "source": [
    "### 100 Epcohs With 4 Attention Heads:\n",
    "```\n",
    "Accuracy: 0.95\n",
    "Loss: 0.15\n",
    "Total Time To Complete: 21028.84 Seconds\n",
    "\n",
    "Generated Text:\n",
    "henry the horse dances the waltz \n",
    " the band begins at ten to six \n",
    " when mr k performs his tricks without a sound \n",
    " and mr h will demonstrate \n",
    " ten summersets hell undertake on solid ground \n",
    " having been some days in preparation \n",
    " a splendid time is guaranteed for all \n",
    " and tonight mr kite is topping the bill \n",
    " birthday \n",
    " you say its your birthday \n",
    " its my birthday tooyeah \n",
    " they say its your birthday \n",
    " were gonna have a good time \n",
    " im glad its your birthday \n",
    " happy birthday to you \n",
    " yes were going to a party party \n",
    " yes were going to a party party \n",
    " yes were going to a party party \n",
    " i would like you to dancebirthday \n",
    " think its your birthday \n",
    " happy birthday to you \n",
    " blackbird \n",
    " blackbird singing in the dead of night \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a8b4073",
   "metadata": {},
   "source": [
    "Based on the results, it is evident that the models have some ability to produce coherent and meaningful content. However, there are some differences in the quality of the generated text across different training epochs and attention heads. \n",
    "\n",
    "In general, the models with more epochs tend to generate more coherent and accurate lyrics. For example, the 50 and 100 epoch models with 2 and 4 attention heads produced more coherent lyrics with a higher acuracy compared to the 1 epoch models.\n",
    "\n",
    "Moreover, the 4 attention head models generally produced more coherent lyrics compared to 2 attention head models. This is most likely do to the increased model complexity.\n",
    "\n",
    "Overall, there is still issues with the generated content. In some cases, the lyrics that the model produced lacks coherence and does not make much sense. In addition, the models will repeat phrases and words multiple time, which can be repetitive and uninteresting. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c41dc86",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In conclusion, these models are far from being good at producing Beatle songs. This could be do to the small set of data trained on or how the training data was represented or ever the combination of both. That being the case, the models were able to produce lyrics that are somewhat coherent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3812e555",
   "metadata": {},
   "source": [
    "## How to Run Code\n",
    "For this project the following python version and libraries used are as follows:\n",
    "```\n",
    "Python=3.11.3\n",
    "Tensorflow=2.12\n",
    "re=2.21\n",
    "time=built-in\n",
    "```\n",
    "\n",
    "Once these are installed, simply just run the cells from top to bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1aee54",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
